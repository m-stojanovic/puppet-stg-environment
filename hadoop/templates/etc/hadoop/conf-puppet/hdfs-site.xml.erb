<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
		<name>dfs.nameservices</name>
		<value><%= scope['hadoop::master_cluster_dfs'] %></value>
	</property>
	<property>
		<name>dfs.ha.namenodes.<%= scope['hadoop::master_cluster_dfs'] %></name>
		<value><%= scope['hadoop::config::master_namenode1_alias'] %>,<%= scope['hadoop::config::master_namenode2_alias'] %></value>
	</property>
	<property>
		<name>dfs.namenode.rpc-address.<%= scope['hadoop::master_cluster_dfs'] %>.<%= scope['hadoop::config::master_namenode1_alias'] %></name>
		<value><%= scope['hadoop::config::master_namenode1'] %>:<%= scope['hadoop::config::namenode_rpc_port'] %></value>
	</property>
	<property>
		<name>dfs.namenode.rpc-address.<%= scope['hadoop::master_cluster_dfs'] %>.<%= scope['hadoop::config::master_namenode2_alias'] %></name>
		<value><%= scope['hadoop::config::master_namenode2'] %>:<%= scope['hadoop::config::namenode_rpc_port'] %></value>
	</property>
	<property>
		<name>dfs.namenode.http-address.<%= scope['hadoop::master_cluster_dfs'] %>.<%= scope['hadoop::config::master_namenode1_alias'] %></name>
		<value><%= scope['hadoop::config::master_namenode1'] %>:50070</value>
	</property>
	<property>
		<name>dfs.namenode.http-address.<%= scope['hadoop::master_cluster_dfs'] %>.<%= scope['hadoop::config::master_namenode2_alias'] %></name>
		<value><%= scope['hadoop::config::master_namenode2'] %>:50070</value>
	</property>
	<property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://<%= scope['hadoop::qjournalnodes'] %>/<%= scope['hadoop::master_cluster_dfs'] %></value>
		<description>the location of the shared storage directory</description>
	</property>
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/data_journal/<%= scope['hadoop::master_cluster_dfs'] %>/edits</value>
		<description>the path where the JournalNode daemon will store its local state</description>
	</property>

	<property>
		<name>dfs.client.failover.proxy.provider.<%= scope['hadoop::master_cluster_dfs'] %></name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
	</property>	
	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
	</property>

	<property>
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence<% if scope['hadoop::config::zkfc_sshfence_params'] %>(<%= scope['hadoop::config::zkfc_sshfence_params'] %>)<% end %></value>
		<description>
			These are configured as a carriage-return-separated list, which will be attempted in order until one indicates that fencing has succeeded.
		</description>
	</property>
	<property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/var/lib/hdfs/.ssh/id_rsa</value>
	</property>
	<property>
		<name>dfs.ha.fencing.ssh.connect-timeout</name>
		<value>15000</value>
		<description>
			SSH connection timeout, in milliseconds, to use with the builtin sshfence fencer.
		</description>
	</property>
	<property>
		<name>dfs.datanode.data.dir</name>
		<!--<value>/DATA1/hadoop-datastore/dfs/data,/DATA2/hadoop-datastore/dfs/data,/DATA3/hadoop-datastore/dfs/data,/DATA4/hadoop-datastore/dfs/data</value>-->
		<value><%= scope['hadoop::datanodedatadir'] %></value>
		<final>true</final>
		<description>
			Determines where on the local filesystem an DFS data node
			should store its blocks.  If this is a comma-delimited
			list of directories, then data will be stored in all named
			directories, typically on different devices.
			Directories that do not exist are ignored.
		</description>
	</property>
	<property>
		<name>dfs.datanode.du.reserved</name>
		<value>104857600</value>
		<final>true</final>
		<description>configured to reserve 100mb for non hadoop use</description>
	</property>
	<property>
		<name>dfs.namenode.name.dir</name>
		<value>file:///data_master/fsimage</value>
		<final>true</final>
		<description>
			Determines where on the local filesystem the DFS name node
			should store the name table(fsimage).  If this is a comma-delimited list
			of directories then the name table is replicated in all of the
			directories, for redundancy.
		</description>
	</property>
	<property>
		<name>dfs.permissions</name>
		<value>true</value>
		<final>true</final>
		<description>
			If "true", enable permission checking in HDFS.
			If "false", permission checking is turned off,
			but all other behavior is unchanged.
			Switching from one parameter value to the other does not change the mode,
			owner or group of files or directories.
		</description>
	</property>
	<property>
	    <name>dfs.permissions.supergroup</name>
	    <value>supergroup</value>
	    <description>The name of the group of super-users.</description>
  	</property>
	<property>
		<name>dfs.blocksize</name>
		<value>134217728</value>
		<final>true</final>
		<description>The default block size for new files.</description>
	</property>
	<property>
		<name>dfs.namenode.handler.count</name>
		<value>32</value>
		<final>true</final>
		<description>The number of server threads for the namenode. Default 10. For new hdp1000 cluster from 16 to 32</description>
	</property>
	<property>
		<name>dfs.datanode.handler.count</name>
		<value>10</value>
		<description>
			The number of server threads for the datanode.
			FAQ:Try increasing it to 10, then by additional increments of 10.
			no sense to use a value larger than the total number of nodes
			in the cluster
			Cloudera suggests 3 here!!!
		</description>
	</property>
	<property>
		<name>dfs.replication</name>
		<value>3</value>
		<description>
			Default block replication.
			The actual number of replications can be specified when the file is created.
			The default is used if replication is not specified in create time.
		</description>
	</property>
	<property>
		<name>dfs.hosts.exclude</name>
		<value>/etc/hadoop/conf/exclude-nodes</value>
		<description>
			Names a file that contains a list of hosts that are
			not permitted to connect to the namenode.  The full pathname of the
			file must be specified.  If the value is empty, no hosts are
			excluded.
			This is very important for decommissioning nodes
		</description>
	</property>

	<property>
		<name>dfs.datanode.max.transfer.threads</name>
		<value>8192</value>
		<description>The hbase book suggests 4096....</description>
	</property>

	<property>
		<name>dfs.datanode.failed.volumes.tolerated</name>
		<value><%= scope['hadoop::config::dfs_datanode_failed_volumes_tolerated'] %></value>
		<final>true</final>
	</property>

	<property>
		<name>dfs.datanode.fsdataset.volume.choosing.policy</name>
		<value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
		<description>
			DN take into account available disk space on each volume when 
			choosing where to place a replica when performing an HDFS write.
		</description>
	</property>

	<!-- MA 04-09-2013 Best practices for HDFS Configuration - Improve Performance for Local Reads -->
        <property>
                <name>dfs.client.read.shortcircuit</name>
                <value><%= scope['hadoop::config::dfs_client_read_shortcircuit'] %></value>
        </property>

        <property>
                <name>dfs.client.read.shortcircuit.buffer.size</name>
                <value><%= scope['hadoop::config::dfs_client_read_shortcircuit_buffer_size'] %></value>
        </property>

        <property>
                <name>dfs.client.read.shortcircuit.streams.cache.size</name>
                <value><%= scope['hadoop::config::dfs_client_read_shortcircuit_streams_cache_size'] %></value>
        </property>

        <property>
                <name>dfs.client.read.shortcircuit.streams.cache.size.expiry.ms</name>
		<value><%= scope['hadoop::config::dfs_client_read_shortcircuit_streams_cache_size_expiry_ms'] %></value>
        </property>

	<property>
		<name>dfs.domain.socket.path</name>
		<value>/var/run/hadoop-hdfs/dn._PORT</value>
	</property>
	<!-- END - MA 04-09-2013 Best practices for HDFS Configuration - Improve Performance for Local Reads -->
	
	<!-- MA 18/09/2013 Increase bandwith for balancing, decrease time for rolling HA logs -->
	<property>
		<name>dfs.ha.log-roll.period</name>
		<value>90</value>
		<description>
		    How often, in seconds, the StandbyNode should ask the active to
		    roll edit logs. Since the StandbyNode only reads from finalized
		    log segments, the StandbyNode will only be as up-to-date as how
		    often the logs are rolled. Note that failover triggers a log roll
		    so the StandbyNode will be up to date before it becomes active.
		</description>
	</property>

	<property>
		<name>dfs.ha.tail-edits.period</name>
		<value>45</value>
		<description>
		    How often, in seconds, the StandbyNode should check for new
		    finalized log segments in the shared edits log.
		</description>
	</property>

	<property>
		<name>dfs.datanode.balance.bandwidthPerSec</name>
		<value>10485760</value>
		<description>
		    Specifies the maximum amount of bandwidth that each datanode
		    can utilize for the balancing purpose in term of
		    the number of bytes per second. 
		</description>
	</property>

<!-- Enabling webhdfs for HUE -->
    <property>
      <name>dfs.webhdfs.enabled</name>
      <value>true</value>
    </property>
<!-- END  MA 18/09/2013 Increase bandwith for balancing, decrease time for rolling HA logs -->

</configuration>
