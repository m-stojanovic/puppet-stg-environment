<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

    <property>
      <name>yarn.application.classpath</name>
      <value>
        $HADOOP_CONF_DIR,
        $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
        $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,
        $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,
        $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*
      </value>
      <description>Classpath for typical applications.</description>
    </property>

<!-- Resource manager -->
    <property>
      <name>yarn.resourcemanager.connect.retry-interval.ms</name>
      <value>2000</value>
    </property>
    <property>
      <name>yarn.resourcemanager.ha.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.resourcemanager.ha.automatic-failover.embedded</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.resourcemanager.cluster-id</name>
      <value><%= scope['hadoop::master_cluster_dfs'] %>_yarn</value>
    </property>
    <property>
      <name>yarn.resourcemanager.ha.rm-ids</name>
      <value><%= scope['hadoop::config::master_jobtracker1_alias'] %>,<%= scope['hadoop::config::master_jobtracker2_alias'] %></value>
    </property>
    <property>
      <name>yarn.resourcemanager.ha.id</name>
     <% if scope['hadoop::config::master_historyserver'] == @fqdn %>
       <value><%= scope['hadoop::config::master_jobtracker2_alias'] %></value>
     <% else %>
       <value><%= scope['hadoop::config::master_jobtracker1_alias'] %></value>
     <% end %>
    </property>
    <property>
      <name>yarn.resourcemanager.scheduler.class</name>
      <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
    </property>
    <property>
      <name>yarn.resourcemanager.recovery.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.resourcemanager.store.class</name>
      <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
      <name>yarn.resourcemanager.zk-address</name>
      <value><%= scope['hadoop::hd_zookeeper_quorum'] %></value>
    </property>
    <property>
      <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
      <value>5000</value>
    </property>
    <property>
      <name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.resourcemanager.am.max-attempts</name>
      <value><%= scope['hadoop::config::yarn_resourcemanager_am_max_attempts'] %></value>
    </property>

    <!-- RM1 configs -->
    <property>
        <name>yarn.resourcemanager.hostname.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
        <value><%= scope['hadoop::config::master_resourcemanager'] %></value>
    </property>
    <property>
      <name>yarn.resourcemanager.address.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
      <value><%= scope['hadoop::config::master_resourcemanager'] %>:8032</value>
    </property>
    <property>
      <name>yarn.resourcemanager.scheduler.address.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
      <value><%= scope['hadoop::config::master_resourcemanager'] %>:8030</value>
    </property>
    <property>
      <name>yarn.resourcemanager.webapp.https.address.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
      <value><%= scope['hadoop::config::master_resourcemanager'] %>:8090</value>
    </property>
    <property>
      <name>yarn.resourcemanager.webapp.address.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
      <value><%= scope['hadoop::config::master_resourcemanager'] %>:50060</value>
    </property>
    <property>
      <name>yarn.resourcemanager.resource-tracker.address.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
      <value><%= scope['hadoop::config::master_resourcemanager'] %>:8031</value>
    </property>
    <property>
      <name>yarn.resourcemanager.admin.address.<%= scope['hadoop::config::master_jobtracker1_alias'] %></name>
      <value><%= scope['hadoop::config::master_resourcemanager'] %>:8033</value>
    </property>

    <!-- RM2 configs -->
    <property>
        <name>yarn.resourcemanager.hostname.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
        <value><%= scope['hadoop::config::master_historyserver'] %></value>
    </property>
    <property>
      <name>yarn.resourcemanager.address.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:8032</value>
    </property>
    <property>
      <name>yarn.resourcemanager.scheduler.address.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:8030</value>
    </property>
    <property>
      <name>yarn.resourcemanager.webapp.https.address.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:8090</value>
    </property>
    <property>
      <name>yarn.resourcemanager.webapp.address.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:50060</value>
    </property>
    <property>
      <name>yarn.resourcemanager.resource-tracker.address.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:8031</value>
    </property>
    <property>
      <name>yarn.resourcemanager.admin.address.<%= scope['hadoop::config::master_jobtracker2_alias'] %></name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:8033</value>
    </property>

   <!-- Node Manager Configs -->
    <property>
      <description>Address where the localizer IPC is.</description>
      <name>yarn.nodemanager.localizer.address</name>
      <value>0.0.0.0:8044</value>
    </property>
    <property>
      <description>NM Webapp address.</description>
      <name>yarn.nodemanager.webapp.address</name>
      <value>0.0.0.0:8042</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.local-dirs</name>
      <value><%= scope['hadoop::yarnlocaldirs'] %></value>
    </property>
    <property>
      <name>yarn.nodemanager.log-dirs</name>
      <value><%= scope['hadoop::yarnlogdirs'] %></value>
    </property>
    <property>
      <name>mapreduce.shuffle.port</name>
      <value>23080</value>
    </property>
    <property>
      <name>yarn.resourcemanager.work-preserving-recovery.enabled</name>
      <value>true</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>

    <!-- History server -->
    <property>
      <name>mapreduce.jobhistory.address</name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:10020</value>
      <description>
      </description>
    </property>
    <property>
      <name>mapreduce.jobhistory.webapp.address</name>
      <value><%= scope['hadoop::config::master_historyserver'] %>:19888</value>
      <description>
      </description>
    </property>
    <property>
      <name>hadoop.proxyuser.mapred.groups</name>
      <value>*</value>
      <description>
      </description>
    </property>
    <property>
      <name>hadoop.proxyuser.mapred.hosts</name>
      <value>*</value>
      <description>
      </description>
    </property>

    <!-- Compression -->
    <property>
      <name>mapreduce.map.output.compress</name>
      <value>true</value>
    </property>
    <property>
      <name>mapred.map.output.compress.codec</name>
      <value>org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>

    <!-- Memory settings -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <description> #num containers * 2GB per container</description>
        <value><%= scope['hadoop::config::yarn_nodemanager_resource_memory_mb'] %></value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <description> 2GB per container</description>
        <value><%= scope['hadoop::config::yarn_scheduler_minimum_allocation_mb'] %></value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <description> #num containers * 2GB per container</description>
        <value><%= scope['hadoop::config::yarn_scheduler_maximum_allocation_mb'] %></value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <description> n * 2GB per container</description>
        <value><%= scope['hadoop::config::yarn_app_mapreduce_am_resource_mb'] %></value>
    </property>
    <!--property>
        <name>yarn.app.mapreduce.am.command-opts</name>
        <description> 0.8 * 2 * 2GB per container</description>
        <value>3600</value>
    </property-->

<!-- Log aggregation -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value><%= scope['hadoop::config::yarn_log_aggregation_enable'] %></value>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value><%= scope['hadoop::config::yarn_nodemanager_remote_app_log_dir'] %></value>
        <description>Where to aggregate logs</description>
    </property>

    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>

    <property>
        <name>yarn.log-aggregation.retain-check-interval-seconds</name>
        <value>3600</value>
    </property>
    <property>
        <name>yarn.nodemanager.log.retain-seconds</name>
        <value>172800</value>
    </property>
    <property>
        <name>yarn.nodemanager.delete.thread-count</name>
        <value>6</value>
    </property>
    <property>
        <name>yarn.nodemanager.delete.debug-delay-sec</name>
        <value>86400</value>
    </property>

    <property>
        <name>yarn.resourcemanager.zk-state-store.parent-path</name>
        <value><%= scope['hadoop::config::yarn_resourcemanager_zk_state_store_parent_path'] %></value>
    </property>

<!-- Mladen test -->

    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value><%= scope['hadoop::config::yarn_nodemanager_resource_cpu_vcores'] %></value>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
        <value>95.0</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value><%= scope['hadoop::config::yarn_scheduler_maximum_allocation_vcores'] %></value>
    </property>
    <property>
      <name>hadoop.proxyuser.hue.groups</name>
      <value>*</value>
      <description>
      </description>
    </property>
    <property>
      <name>hadoop.proxyuser.hue.hosts</name>
      <value>*</value>
      <description>
      </description>
    </property>
        <property>
      <name>hadoop.proxyuser.yarn.groups</name>
      <value>*</value>
      <description>
      </description>
    </property>
    <property>
      <name>hadoop.proxyuser.yarn.hosts</name>
      <value>*</value>
      <description>
      </description>
    </property>
</configuration>
