<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
	<name>fs.defaultFS</name>
	<value>hdfs://<%= scope['hadoop::master_cluster_dfs'] %></value>
	<description>
		The name of the default file system.  A URI whose
		scheme and authority determine the FileSystem implementation.  The
		uri's scheme determines the config property (fs.SCHEME.impl) naming
		the FileSystem implementation class.  The uri's authority is used to
		determine the host, port, etc. for a filesystem.
	</description>
        <property>
        <name>fs.default.name</name>
        <value>hdfs://<%= scope['hadoop::master_cluster_dfs'] %></value>
    </property>
</property>
<property>
	<name>ha.zookeeper.quorum</name>
	<value><%= scope['hadoop::hd_zookeeper_quorum'] %></value>
	<description>
		A list of ZooKeeper server addresses, separated by commas, that are
		to be used by the ZKFailoverController in automatic failover.
	</description>
</property>
<property>
	<name>io.file.buffer.size</name>
	<value>65536</value>
	<description>
		The size of buffer for use in sequence files.
		The size of this buffer should probably be a multiple of hardware
		page size (4096 on Intel x86), and it determines how much data is
		buffered during read and write operations.
	</description>
</property>
<property>
	<name>fs.trash.interval</name>
	<value>1440</value>
	<description>
		Number of minutes between trash checkpoints.
		If zero, the trash feature is disabled.
	</description>
</property>
<property>
	<name>io.compression.codecs</name>
	<value>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec</value>
	<description>
		A list of the compression codec classes that can be used
		for compression/decompression.
	</description>
</property>
<property>
	<name>io.compression.codec.lzo.class</name>
	<value>com.hadoop.compression.lzo.LzoCodec</value>
</property>
<property>
	<name>net.topology.script.file.name</name>
	<value>/etc/hadoop/conf/topology.py</value>
</property>
<property>
   <name>hadoop.http.staticuser.user</name>
   <value>yarn</value>
</property>

<!-- Hue WebHDFS proxy user setting -->
<property>
  <name>hadoop.proxyuser.hue.hosts</name>
  <value>*</value>
</property>
<property>
  <name>hadoop.proxyuser.hue.groups</name>
  <value>*</value>
</property>
<property>  
  <name>hadoop.proxyuser.httpfs.hosts</name>  
  <value>*</value>  
</property>  
<property>  
<name>hadoop.proxyuser.httpfs.groups</name>  
  <value>*</value>  
</property>  

</configuration>
